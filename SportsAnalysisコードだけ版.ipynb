{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SportsAnalysisコードだけ版.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taniokah/Sports_Analysis/blob/master/SportsAnalysis%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A0%E3%81%91%E7%89%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJHUKxeJISqP",
        "colab_type": "text"
      },
      "source": [
        "# お詫び\n",
        "諸般の事情でコードだけの公開と相成りました．コメントは出来る限り入れておりますが，不明な部分も多いかと存じます．よろしくお願い致します．急遽公開ということで，突貫工事で作業したので，つながりが変な部分もあるかもしれません…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8XNqtodJQGg",
        "colab_type": "text"
      },
      "source": [
        "# コード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmPlMs3vLL-p",
        "colab_type": "text"
      },
      "source": [
        "## Colaboratory内での動画表示\n",
        "参照元：https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/OpenPose.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl7TL-XlRiJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XKJib8SQXO5",
        "colab_type": "text"
      },
      "source": [
        "動画を表示する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-D-jJtrNlJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_local_mp4_video('hoge.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfG0wKlBPdxb",
        "colab_type": "text"
      },
      "source": [
        "## 局所特徴量を用いたパノラマ画像への各フレームの画像のマッチング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JagifjDRL0q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#マッチングさせる動画fen.mp4\n",
        "#パノラマ画像back.png\n",
        "#area.mp4で書き出し\n",
        "\n",
        "#ライブラリのロード\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 動画作成\n",
        "fourcc = cv2.VideoWriter_fourcc('m','p','4', 'v')\n",
        "video  = cv2.VideoWriter('area.mp4', fourcc, 30.0, (1750, 747))\n",
        "\n",
        "#上位何%の特徴点を信頼するか\n",
        "match_rate = 0.5\n",
        "\n",
        "#フレーム番号確認用カウント\n",
        "count = 0\n",
        "\n",
        "# 読み込み動画\n",
        "cap = cv2.VideoCapture('fen.mp4')\n",
        "\n",
        "# マッチング&動画書き込み\n",
        "while True:\n",
        "  #動画をフレームごとに読み込み\n",
        "  ret, image = cap.read()\n",
        "  if ret == True:\n",
        "    count += 1\n",
        "    # 50フレームごとにフレーム番号を表示\n",
        "    if count%50==0:\n",
        "      print(count)\n",
        "    # パノラマ画像\n",
        "    img1 = cv2.imread('./back.png',0)\n",
        "    img2 = image\n",
        "    # 特徴点抽出にAKAZE法を使用\n",
        "    akz = cv2.AKAZE_create()\n",
        "    # AKAZE法によりimg1, img2の画像特徴点を計算する\n",
        "    kp1, des1 = akz.detectAndCompute(img1,None)\n",
        "    kp2, des2 = akz.detectAndCompute(img2,None)\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    # 2つの画像の特徴点をマッチさせる\n",
        "    matches = bf.match(des1, des2)\n",
        "    # 上位match_rate％の特徴量を確保\n",
        "    matches = sorted(matches, key=lambda x: x.distance)\n",
        "    m_points = matches[:int(len(matches) * match_rate)]\n",
        "    # 特徴量をもとに画像を変換\n",
        "    src_pts = np.float32([kp1[m.queryIdx].pt for m in m_points]).reshape(-1, 1, 2)\n",
        "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in m_points]).reshape(-1, 1, 2)\n",
        "    # 透視変換行列の計算\n",
        "    h, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC)\n",
        "    # フレーム画像を変換\n",
        "    height, width = img1.shape[0:2]\n",
        "    dst_img = cv2.warpPerspective(img2, h, (width, height))\n",
        "    # 書き出し\n",
        "    video.write(dst_img)\n",
        "  else:\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJhLPSRdPowR",
        "colab_type": "text"
      },
      "source": [
        "### 結果\n",
        "AKAZE特徴量による各フレームのパノラマ画像への割り振りによって，カメラのパン状態が推定できている\n",
        "\n",
        "（特徴点の少なくなる右側だと，推定精度が若干落ちるが，ご愛嬌ということで…）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfB6MbOdw9wn",
        "colab_type": "text"
      },
      "source": [
        "●動画の変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F9ivK3lOGN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ffmpeg -y -v error -i area.mp4 area_c.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCpqxPQASA1S",
        "colab_type": "text"
      },
      "source": [
        "●動画の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyPtoK6Dn-uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_local_mp4_video('area_c.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG02WKgAUJCN",
        "colab_type": "text"
      },
      "source": [
        "## 対象の姿勢推定（ここは多分大丈夫なはず）\n",
        "映像からのスポーツデータ解析において，選手の姿勢を推定し，評価することは一つの興味の対象です．ここでは映像からの姿勢推定アルゴリズムの一つであるPosenetを利用して，選手の姿勢推定を行う方法について紹介します．\n",
        "\n",
        "### 映像からの姿勢推定技術\n",
        "映像からの姿勢推定技術に関する研究は，長い間行われてきましたが，近年急激に躍進した印象があります．代表的なアルゴリズム（ソフトウェア）としては，OpenPose，VNect，DensePose, PoseNetなどがあるが，OpenPoseはライセンスの問題で扱いづらく，VNectは学習モデルの提供に交渉が必要，またDensPoseは3Dのメッシュ情報を推定するのが目的となっているので，今回はPoseNetを使用する．\n",
        "\n",
        "### PoseNet\n",
        "PoseNetは，Googleが提供している姿勢推定ソフトウェアで，画像から対象の姿勢を推定することが可能（あくまで画像上の関節構造であって，3次元の姿勢情報は提供されないことに注意）．簡単なお試しであれば，\n",
        "\n",
        "https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html\n",
        "\n",
        "から体験可能．今回は，これをPythonから手軽に扱えるライブラリposenet-pytorch\n",
        "\n",
        "https://github.com/rwightman/posenet-pytorch\n",
        "\n",
        "を用いて，姿勢推定を行う．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGzxxn_m2U3T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "posenet-pytorchは画像単位の姿勢推定なので，まず，動画データをフレーム毎の画像に変換して，imgディレクトリに格納する．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5O0p9YraFQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#area_c.mp4を用いてposenetに適用\n",
        "!mkdir img\n",
        "!ffmpeg -v error -i area_c.mp4 -r 30 -f image2 ./img/%08d.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxF1fK2f2eRy",
        "colab_type": "text"
      },
      "source": [
        "posenet-pytorchディレクトリに移動する．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqw-TYgFJkKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://lab.0093.tv/sports/posenet-pytorch.zip\n",
        "!unzip posenet-pytorch.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVI6QpVwy_6Q",
        "colab_type": "code",
        "outputId": "4639ab2a-c374-4da0-8952-4d09882d535b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd posenet-pytorch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/posenet-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y2dW53_2z1R",
        "colab_type": "text"
      },
      "source": [
        "### PoseNetによる姿勢推定\n",
        "もともとのposenet-pytorchライブラリのimage_demo.pyを元に改良．\n",
        "パラメータの調整をしやすくするため，デフォルトにはないオプションを幾つか追加している．また，パラメータ調整以外にも，各関節座標の情報をjsonファイルとしてjsondirに格納するようにしている（デフォルトでは座標をコンソールに出力するのみ．notxtオプションをつけた場合は，出力しない）．\n",
        "\n",
        "param1, param2の値で結果が結構変わってくるので，実用時は要チューニング．\n",
        "\n",
        "* param1: 姿勢の推定確率の閾値\n",
        "* param2: 部位の推定確率の閾値\n",
        "* maxp: 推定する最大人数\n",
        "* model: mobilenet_v1を使用している．デフォルトではバージョン？101\n",
        "* image_dir: 入力する画像を格納しているディレクトリ\n",
        "* output_dir: スケルトンモデルを上書きした画像を出力するディレクトリ\n",
        "* notxt: このオプションが指定されたときは，座標情報を出力しない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE0oLh0b-G4L",
        "colab_type": "text"
      },
      "source": [
        "Posenetでは，計17箇所の関節の推定結果を返す．\n",
        "\n",
        "![Posenetの関節座標](https://cdn-images-1.medium.com/max/800/1*7qDyLpIT-3s4ylULsrnz8A.png)\n",
        "（参照元：\n",
        "https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n",
        "）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRLrid-smyTW",
        "colab_type": "code",
        "outputId": "7e7c846e-522e-411e-ff34-aae97fcd252f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python image.py --param1 0.05 --param2 0.1 --maxp=5 --model 101 --image_dir ../img --output_dir ./out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average FPS: 8.070821877852536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS-oSvWb9WcR",
        "colab_type": "text"
      },
      "source": [
        "### 結果\n",
        "Posenetによる姿勢推定の結果，ところどころ当てはまりが悪いところがあるが，関節の推定はおおよそうまく行っていることがわかる．\n",
        "\n",
        "ただし，ここで推定された関節の座標は，2次元上で推定された座標であって，3次元の空間座標ではないことに注意する．\n",
        "\n",
        "2次元の関節座標から，3次元への座標を推定する方法としては，\n",
        "\n",
        "https://github.com/una-dinosauria/3d-pose-baseline\n",
        "\n",
        "があるが，セットアップが大変なのと，計算時間がかかるので，ここでは取り扱わない．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JqxkpGi7Wj4",
        "colab_type": "text"
      },
      "source": [
        "●動画の変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O9m_J-9unbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ffmpeg -v error -i ./out/%08d.png -r 30 out.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjx-cf2Q7XT4",
        "colab_type": "text"
      },
      "source": [
        "●動画の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9PRr7o3lS8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_local_mp4_video('out.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-T2RU6g7dvE",
        "colab_type": "text"
      },
      "source": [
        "## ピスト（コート）の推定\n",
        "ピスト（コート）の推定は，各フレームでの対象の位置座標を推定するのに重要になる．フェンシングのピストは以下のような構造をしており，ピストは2本の平行な直線が各領域で分割された形であることが見て取れる．\n",
        "\n",
        "![代替テキスト](http://www.fencingfan.net/communications/images/piste_1_b.jpg)\n",
        "（参照元：\n",
        "http://www.fencingfan.net/communications/\n",
        "）\n",
        "\n",
        "ここではHough変換による直線検出を行うことによってピストの推定を行う．\n",
        "\n",
        "\n",
        "###Hough変換\n",
        "Hough変換は，古典的ではあるが有効な直線や円の検出方法として今でも用いられている．身近な利用例では（実際に使われているかどうかは分からないが…）運転時の車線検出なども，この手法を用いることで可能である．\n",
        "\n",
        "一般的には一度エッジ検出を行い，画像内の物体の輪郭を抽出してから，Hough変換を行うことによって，直線を検出することが多い．\n",
        "\n",
        "実際には映像中の直線要素は数多く存在することから，今回は対象となるコート（画面下側）のみを対象とし，直線抽出を行った．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvKHX-4OhoQA",
        "colab_type": "text"
      },
      "source": [
        "posenet-pytorchのディレクトリから移動する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2kJWvBv8deY",
        "colab_type": "code",
        "outputId": "79a24198-510b-4e31-dd94-5f7d49d35e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4hN3xzKBlrt",
        "colab_type": "text"
      },
      "source": [
        "映像中からの直線の検出\n",
        "（コートの色情報を抽出し，Hough変換により直線を抽出する）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIUxWp5PpFUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2点(x1,y1)(x2,y2)から任意の座標xが与えられたときのy座標を返す\n",
        "def liney(x,x1,x2,y1,y2):\n",
        "  y = ((y2-y1)/(x2-x1))*(x-x1) + y1\n",
        "  return(y)\n",
        "\n",
        "#必要ライブラリのロード\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 読み込み動画\n",
        "cap = cv2.VideoCapture('area_c.mp4')\n",
        "\n",
        "#対象とするコートの色情報\n",
        "bgrLower = np.array([40, 100, 170])    # 抽出する色の下限(BGR)\n",
        "bgrUpper = np.array([110, 180, 250])    # 抽出する色の上限(BGR)\n",
        "\n",
        "# 抽出された色部分から，直線推定\n",
        "# resの中にx=1およびx=1750でのy座標を格納する．\n",
        "res = list()\n",
        "while True:\n",
        "  #動画をフレームごとに読み込み\n",
        "  ret, image = cap.read()\n",
        "  if ret == True:\n",
        "    image2 = image.copy()\n",
        "    #対象とするコートに関係のない部分（上半分）を黒で塗りつぶし\n",
        "    cv2.rectangle(image, (1, 1), (1750, 420), (0, 0, 0), thickness=-1)\n",
        "    #対象とする色部分を画像から抽出\n",
        "    mask = cv2.inRange(image, bgrLower, bgrUpper)\n",
        "    img2 = cv2.bitwise_and(image, image, mask=mask)\n",
        "    #グレースケールに変換\n",
        "    hsv = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "    #エッジ検出\n",
        "    edges = cv2.Canny(hsv, 200, 250)\n",
        "    #Hough変換による直線検出\n",
        "    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 90, minLineLength = 350, maxLineGap=100)\n",
        "    #推定された直線のx=1とx=1750でのy座標を格納\n",
        "    tmplist = list()\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            x1, y1, x2, y2 = line[0]\n",
        "            if (y1 > 420) and (y2 > 420):\n",
        "              tmplist.append(int(liney(1,x1,x2,y1,y2)))\n",
        "              tmplist.append(int(liney(1750,x1,x2,y1,y2)))\n",
        "    res.append(tmplist)\n",
        "  else:\n",
        "    break\n",
        "\n",
        "cap.release()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMYUfgBLfw7v",
        "colab_type": "text"
      },
      "source": [
        "Hough変換による直線検出は一本の直線を複数の直線と検出してしまうことがあるので，ピストの中心を境に上側と下側の推定されたy座標の中央値を取ることによって，一つの点とする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBhh_-IHqyZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 直線の統一化\n",
        "def calpoints(p):\n",
        "  # 得られたx=1とx=1750でのy座標の値を行列化する．\n",
        "  # 1列目をx=1のときのy座標，2列目をx=1750のときのy座標として行列を構築する\n",
        "  tmp = np.array(p)\n",
        "  ld = int(len(tmp)/2)\n",
        "  tmp = tmp.reshape((ld, 2))\n",
        "  # 1列目(x=1)の最大値と最小値およびその中間点を計算する．\n",
        "  pmi = np.min(tmp[::,0])\n",
        "  pma = np.max(tmp[::,0])\n",
        "  b = (pmi+pma)/2\n",
        "  # 中間点より上側および下側の点のy座標の中央値をそれぞれlu, llとする．\n",
        "  lu = np.round(np.median(tmp[tmp[::,0] > b,0]))\n",
        "  ll = np.round(np.median(tmp[tmp[::,0] < b,0]))\n",
        "  # 2列目(x=1750)の最大値と最小値およびその中間点を計算する．\n",
        "  pmi = np.min(tmp[::,1])\n",
        "  pma = np.max(tmp[::,1])\n",
        "  b = (pmi+pma)/2\n",
        "  # 中間点より上側および下側の点のy座標の中央値をそれぞれru, rlとする．\n",
        "  ru = np.round(np.median(tmp[tmp[::,1] > b,1]))\n",
        "  rl = np.round(np.median(tmp[tmp[::,1] < b,1]))\n",
        "  # ↑の結果をresに格納し，返り値とする．\n",
        "  res = [lu,ru,ll,rl]\n",
        "  return(res)\n",
        "\n",
        "# それぞれのフレームごとに↑の計算を行い，ppに格納する．\n",
        "pp = list()\n",
        "for i in range(len(res)):\n",
        "  pp.append(calpoints(res[i]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N1cpNWjn3fB",
        "colab_type": "text"
      },
      "source": [
        "↑で計算された座標を元に，映像に直線を描く"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsRUxYforE1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ライブラリのロード\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 動画作成\n",
        "fourcc = cv2.VideoWriter_fourcc('m','p','4', 'v')\n",
        "video  = cv2.VideoWriter('line.mp4', fourcc, 30.0, (1750, 747))\n",
        "\n",
        "# 読み込み動画\n",
        "cap = cv2.VideoCapture('area_c.mp4')\n",
        "while True:\n",
        "  # 動画をフレームごとに読み込み\n",
        "  ret, image = cap.read()\n",
        "  if ret == True:\n",
        "    #上側および下側の直線の描写\n",
        "    cv2.line(image, (1, int(pp[count][0])), (1750, int(pp[count][1])), (0, 0, 255), 5)\n",
        "    cv2.line(image, (1, int(pp[count][2])), (1750, int(pp[count][3])), (0, 0, 255), 5)\n",
        "    video.write(image)\n",
        "  else:\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQh-H0V3DDIK",
        "colab_type": "text"
      },
      "source": [
        "### 結果\n",
        "Hough変換を用いることにより，コートの直線の検出を行った．今回はコートの色や領域などの事前情報を用いることでHough変換で検出しやすくしている．近年では深層学習を用いた直線検出なども提案されてきており，Hough変換はお手軽に計算できるので，利点は多いものの，今後はそれらの方法が一般化する可能性がある．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh_eV0WVoSrP",
        "colab_type": "text"
      },
      "source": [
        "●動画の変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGX1KXmMrVwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ffmpeg -y -v error -i line.mp4 line2.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_C-KEd5oWSP",
        "colab_type": "text"
      },
      "source": [
        "●動画の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_iwSYX7rcP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_local_mp4_video('line2.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eIvoBvPovk5",
        "colab_type": "text"
      },
      "source": [
        "## 透視投影変換行列\n",
        "選手の座標を平面のコート上に表示するために，透視投影変換行列の計算を行う．今回パンを含むカメラの映像から，左側の開始線と，右側のエンドラインの内側が画面内に収まっており，この情報を使用することによって，透視投影変換行列の計算を行う．\n",
        "\n",
        "具体的には左側の開始線と，右側のエンドラインの内側は，2m x 7mのエリアとなるので，パノラマ画像を用いて，その変換を行う．\n",
        "（※事前にパノラマ画像における開始線の上側，下側，エンドラインの上側，下側の座標は手動で取得しておく）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnPCJxD6rDqg",
        "colab_type": "text"
      },
      "source": [
        "画像表示のためのライブラリをロードする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29oclSA8gH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcgrj5ZbrLYh",
        "colab_type": "text"
      },
      "source": [
        "透視投影変換行列の計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcTUpxiY1PoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ライブラリのロード\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 背景のパノラマ画像の読み込み\n",
        "img = cv2.imread('./back.png') \n",
        "# 変換前の4隅の座標を入力\n",
        "src_pts=np.float32([[245, 557], [1572, 506], [237, 665], [1679, 602]])\n",
        "# 変換後の4隅の座標を入力\n",
        "dst_pts=np.float32([[0, 0], [700, 0], [0, 200], [700, 200]])\n",
        "# 変換前変換後の座標を元に，透視投影変換行列を計算\n",
        "M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
        "# 実際にパノラマ画像内の領域を変換\n",
        "dst = cv2.warpPerspective(img, M, (700,200))\n",
        "\n",
        "plt.figure(figsize=(20, 20), dpi=50)\n",
        "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
        "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
        "plt.show()\n",
        "\n",
        "print(\"透視投影変換行列\")\n",
        "print(M)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMkdgN_asbzw",
        "colab_type": "text"
      },
      "source": [
        "### ピスト内の選手の判定\n",
        "透視投影変換行列が得られることにより，映像中の選手の座標も2m x 7mの平面に投影することができる．しかし，映像中には複数の人物が写り込んでいるので，対象とする選手であることを特定するため，Posenetによって推定された足の座標（今回は右足のみとした）がコートを示すに直線の内側にあるか否かで選手の判定を行う．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SB7l2MOUvS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (x1, y1), (x2, y2)の2点を通る直線と，右足の座標の位置関係の計算\n",
        "# ansの値の正負で直線のどちら側にあるか判定することができる．\n",
        "def lined(x1,y1,x2,y2,x,y):\n",
        "  vx1 = x2 - x1\n",
        "  vy1 = y2 - y1\n",
        "  vx2 = x - x1\n",
        "  vy2 = y - y1\n",
        "  ans = vx1 * vy2 - vy1 * vx2\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "# 各フレームにおいて，ピスト内にある選手のIDを取得する\n",
        "# IDを格納するリストの作成\n",
        "# coorにはPosenetで推定された関節座標をまとめたものが入っている．\n",
        "ids = list()\n",
        "# 各フレームで計算を行う．\n",
        "for i in range(len(coor)):\n",
        "  # 一時的な空リストの作成\n",
        "  tmp = list()\n",
        "  # 各フレームで認識されている被験者分判定を行う．\n",
        "  for j in range(len(coor[i])):\n",
        "    # ピストを囲む上下直線の左端(x=1)と右端(x=1750)の座標を取得\n",
        "    pt = np.array([[1,pp[i][0]], [1750,pp[i][1]], [1,pp[i][2]], [1750,pp[i][3]]])\n",
        "    # 被験者番号Jの右足の座標を取得\n",
        "    co = coor[i][j][10][0:2]\n",
        "    # 右足の座標が上下直線のどちら側にあるかを判定\n",
        "    dd1 = lined(pt[0][0],pt[0][1],pt[1][0],pt[1][1],co[0],co[1])\n",
        "    dd2 = lined(pt[2][0],pt[2][1],pt[3][0],pt[3][1],co[0],co[1])\n",
        "    # 符号の判定\n",
        "    tf1 = dd1<0\n",
        "    tf2 = dd2<0\n",
        "    # 符号が一致していない場合，ピスト内にいると判定する\n",
        "    if (tf1==tf2)==False:\n",
        "      tmp.append(j)\n",
        "  ids.append(tmp)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAMwMyZfup2r",
        "colab_type": "text"
      },
      "source": [
        "### 疑似コートへの描写\n",
        "ピスト内の選手の判定結果を元に，ピスト上での選手の座標の描写を行う．今回は右足と左足の座標および，それを直線で結んだものを描写することにする．各足の座標の変換は，先程計算した透視投影変換行列（プログラム中ではM）を使用することによって，得ることができる．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnOVsFC3DCCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ライブラリのロード\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# 動画作成（コートの座標はマージンを取って3m x 9mとした）\n",
        "fourcc = cv2.VideoWriter_fourcc('m','p','4', 'v')\n",
        "video  = cv2.VideoWriter('result.mp4', fourcc, 30.0, (900, 300))\n",
        "\n",
        "# 各フレーム分変換を行う\n",
        "for i in range(len(coor)):\n",
        "  # 空のコートの画像（3m x 9m…300px x 900px）の画像を作成する\n",
        "  img = np.full((300, 900, 3), 200, dtype=np.uint8)\n",
        "  # 上側の直線を引く\n",
        "  cv2.line(img, (1, 50), (900, 50), (255, 255, 255), thickness=2, lineType=cv2.LINE_4)\n",
        "  # 下側の直線を引く\n",
        "  cv2.line(img, (1, 250), (900, 250), (255, 255, 255), thickness=2, lineType=cv2.LINE_4)\n",
        "  # 右側開始線を描く\n",
        "  cv2.rectangle(img, (440, 50), (460, 250), (255, 255, 255), thickness=-1)\n",
        "  # 左側開始線を描く\n",
        "  cv2.rectangle(img, (40, 50), (60, 250), (255, 255, 255), thickness=-1)\n",
        "  # 中心線を描く\n",
        "  cv2.line(img, (250, 50), (250, 250), (255, 255, 255), thickness=5, lineType=cv2.LINE_4)\n",
        "  # エンドラインを描く\n",
        "  cv2.rectangle(img, (750, 50), (900, 250), (255, 255, 255), thickness=-1)\n",
        "  # 足の座標を描く\n",
        "  for j in range(len(ids[i])):\n",
        "    # 該当被験者のIDを取得\n",
        "    id = ids[i][j]\n",
        "    # 右足及び左足の座標を取得\n",
        "    co1 = np.insert(coor[i][id][10][0:2],2,1)\n",
        "    co2 = np.insert(coor[i][id][13][0:2],2,1)\n",
        "    # 行列形式に変換\n",
        "    co1.shape = (3,1)\n",
        "    co2.shape = (3,1)\n",
        "    # 透視投影変換行列によって平面上に投影\n",
        "    co1 = np.dot(M,co1)\n",
        "    co1 = (co1/co1[2]).T[0][0:2]+100\n",
        "    co2 = np.dot(M,co2)\n",
        "    co2 = (co2/co2[2]).T[0][0:2]+100\n",
        "    co1 = tuple(co1.astype(np.int32))\n",
        "    co2 = tuple(co2.astype(np.int32))\n",
        "    # 右足と左足を線で結ぶ\n",
        "    cv2.line(img, co1, co2, (0, 0, 0), thickness=3, lineType=cv2.LINE_4)\n",
        "    # 右足及び左足の座標をプロットする\n",
        "    cv2.circle(img, co1, 10, (255,0,0), thickness=-1)\n",
        "    cv2.circle(img, co2, 10, (255,0,0), thickness=-1)\n",
        "  video.write(img)\n",
        "  \n",
        "video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH8WJwv7xvsT",
        "colab_type": "text"
      },
      "source": [
        "### 結果\n",
        "透視投影変換行列によってコートの座標に変換した結果，各選手の右足と左足の座標の試合中の変化を確認することができた．特に～10sまでは右側の選手が審判の影に隠れていることによって，表示ができていないが，その他の領域については，ノイズが交じるものの，およそプロットすることができている．\n",
        "\n",
        "今回は投影された座標をそのまま表示しているが，実際の解析の際は，座標の変化を平滑化することによって，ノイズの変化を低減することができ，また一部推定できなかった領域については，パーティクルフィルタなどを用いることによって，保管する手法などが考えられる．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irgKSLWLxgwr",
        "colab_type": "text"
      },
      "source": [
        "●動画の変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZAoMTHKJPkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ffmpeg -y -v error -i result.mp4 result2.mp4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkJQfAWgxlie",
        "colab_type": "text"
      },
      "source": [
        "●動画の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcgqEcBWJVFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_local_mp4_video('result2.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}